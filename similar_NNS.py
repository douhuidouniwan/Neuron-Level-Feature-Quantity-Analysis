"""
This script builds upon the outcomes of nnscanner to conduct further experiments.  
It focuses on evaluating the similarity of learning images generated by neural  
activations, aiming to provide quantitative insights into feature representation  
and the quality of learned patterns.
"""

import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import sys
from function.ssimquan import *
from function.utils import *
from function.quanindex import *
import os
import csv


class SimilarMeaNNS():
    def __init__(self, dataname, xmldir,device):
        self.dataname = dataname
        self.xmldir = xmldir
        self.device = device
        self.bbox_dict = {}

        if dataname == 'tinyimg32':
            imglst = os.listdir(self.xmldir)
            for i in range(len(imglst)):
                with open(os.path.join(self.xmldir, imglst[i]), "r") as bbox_file:
                    bbox_data = csv.reader(bbox_file, delimiter="\t")
                    for row in bbox_data:
                        image_name = row[0].split('.')[0]
                        bbox_info = list(map(int, row[1:]))
                        self.bbox_dict[image_name]=bbox_info




    def getforeground(self,cropimg,imgname,scanimg):
        if self.dataname == 'mnist':
            foreimage = cropimg
        if self.dataname == 'imagenet':
            foreimage = torch.zeros(cropimg.size(),map_location=self.device )
            backimage = cropimg
            xdir = imgname[1] + '/'+ imgname[1] + '_' + imgname[2]
            bounding_boxes = load_parse_xml(self.xmldir+'/'+xdir+'.xml')
            x_min, y_min, x_max, y_max = bounding_boxes[0]
            foreimage[:,x_min:x_max,y_min:y_max] = cropimg[:,x_min:x_max,y_min:y_max]
            backimage[:,x_min:x_max,y_min:y_max] = 0
        
        if self.dataname == 'imagenet32': #224/7
            
            xdir = imgname[1] + '/'+ imgname[1] + '_' + imgname[2]
            bounding_boxes = load_parse_xml(self.xmldir+'/'+xdir+'.xml')
            x_min, y_min, x_max, y_max = [a//7 for a in bounding_boxes[0]]
            # print(x_min, y_min, x_max, y_max)
            foreimage = torch.zeros(cropimg.size())
            foreimage = foreimage.to(self.device)
            backimage = cropimg.clone().detach()
            foreimage[:,x_min:x_max,y_min:y_max] = cropimg[:,x_min:x_max,y_min:y_max]
            backimage[:,x_min:x_max,y_min:y_max] = 0

            forescan = torch.zeros(scanimg.size())
            forescan = forescan.to(self.device)
            backscan = scanimg.clone().detach()
            forescan[x_min:x_max,y_min:y_max] = scanimg[x_min:x_max,y_min:y_max]
            backscan[x_min:x_max,y_min:y_max] = 0
            
            # backimage = cropimg - foreimage

        if self.dataname == 'tinyimg32': #34/2
            bounding_boxes = self.bbox_dict[imgname[1] + '_' + imgname[2]]
            x_min, y_min, x_max, y_max = [a//2 for a in bounding_boxes]
            # print(x_min, y_min, x_max, y_max)
            foreimage = torch.zeros(cropimg.size())
            foreimage = foreimage.to(self.device)
            backimage = cropimg.clone().detach()
            foreimage[:,x_min:x_max,y_min:y_max] = cropimg[:,x_min:x_max,y_min:y_max]
            backimage[:,x_min:x_max,y_min:y_max] = 0

            forescan = torch.zeros(scanimg.size())
            forescan = forescan.to(self.device)
            backscan = scanimg.clone().detach()
            forescan[x_min:x_max,y_min:y_max] = scanimg[x_min:x_max,y_min:y_max]
            backscan[x_min:x_max,y_min:y_max] = 0

        return foreimage, backimage,forescan,backscan


    def similar(self,foreimage,backimage,orimage,scimage,forescan,backscan):
        arr = np.zeros(3) 
        arr[0] = quandis(foreimage,forescan)
        arr[1] = quandis(backimage,backscan)
        arr[2] = quandis(orimage,scimage)


        return arr